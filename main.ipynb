{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cliffwalking\n",
      "TD\n",
      "test start\n",
      "Average Rewards: -13.0\n",
      "Success Rate: 100.0%\n",
      "Average Steps to Goal: 13.0\n",
      "q_learning\n",
      "test start\n",
      "Average Rewards: -13.0\n",
      "Success Rate: 100.0%\n",
      "Average Steps to Goal: 13.0\n",
      "sarsa\n",
      "test start\n",
      "Average Rewards: -15.0\n",
      "Success Rate: 100.0%\n",
      "Average Steps to Goal: 15.0\n",
      "frozenlake\n",
      "TD\n",
      "test start\n",
      "Average Rewards: 0.3333333333333333\n",
      "Success Rate: 100.0%\n",
      "Average Steps to Goal: 8.333333333333334\n",
      "q_learning\n",
      "test start\n",
      "Average Rewards: 0.0\n",
      "Success Rate: 100.0%\n",
      "Average Steps to Goal: 12.0\n",
      "sarsa\n",
      "test start\n",
      "Average Rewards: 0.0\n",
      "Success Rate: 100.0%\n",
      "Average Steps to Goal: 9.666666666666666\n",
      "taxi\n",
      "TD\n",
      "test start\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, env, epsilon, q_values, is_dqn=False):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()  # Explore: select a random action\n",
    "    else:\n",
    "        if is_dqn:\n",
    "            return np.argmax(q_values)  # Q-values are already computed for DQN\n",
    "        else:\n",
    "            return np.argmax(q_values[state])  # Q-values from Q-table\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "def one_hot_encode(state, num_states):\n",
    "    state_vector = np.zeros(num_states)\n",
    "    state_vector[state] = 1\n",
    "    return state_vector\n",
    "\n",
    "def train(args):\n",
    "    print(\"train start\")\n",
    "    if args.env_name == 'frozenlake':\n",
    "        env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "    elif args.env_name == 'cliffwalking':\n",
    "        env = gym.make('CliffWalking-v0')\n",
    "    elif args.env_name == 'taxi':\n",
    "        env = gym.make('Taxi-v3')\n",
    "\n",
    "    env.reset()\n",
    "    env.action_space.seed(42)\n",
    "\n",
    "    # Initialize Q-table with zeros\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    if args.model_name == 'q_learning':\n",
    "        alpha = args.alpha  # Learning rate\n",
    "        gamma = args.gamma  # Discount factor\n",
    "        epsilon = args.epsilon  # Exploration rate\n",
    "\n",
    "        num_episodes = args.num_episodes\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = epsilon_greedy_policy(state, env, epsilon, q_table)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                # Update Q-value using the Bellman equation\n",
    "                old_value = q_table[state, action]\n",
    "                next_max = np.max(q_table[next_state])\n",
    "                new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "                q_table[state, action] = new_value\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "\n",
    "            rewards_per_episode.append(episode_rewards)\n",
    "\n",
    "    elif args.model_name == 'MC':\n",
    "        gamma = args.gamma\n",
    "        alpha = args.alpha\n",
    "        epsilon = args.epsilon\n",
    "        num_episodes = args.num_episodes\n",
    "\n",
    "        N = np.zeros((env.observation_space.n, env.action_space.n))  # Visit count for each state-action pair\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            episode_states = []  # States visited in this episode\n",
    "            episode_actions = []  # Actions taken in this episode\n",
    "            episode_rewards = []  # Rewards received in this episode\n",
    "\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = epsilon_greedy_policy(state, env, epsilon, q_table)  # Epsilon-greedy action selection\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                episode_states.append(state)\n",
    "                episode_actions.append(action)\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            total_reward = sum(episode_rewards)\n",
    "            rewards_per_episode.append(total_reward)\n",
    "\n",
    "            # Update Q-values\n",
    "            G = 0\n",
    "            for t in range(len(episode_states) - 1, -1, -1):  # Loop through the episode in reverse\n",
    "                state = episode_states[t]\n",
    "                action = episode_actions[t]\n",
    "                reward = episode_rewards[t]\n",
    "\n",
    "                G = gamma * G + reward\n",
    "                N[state][action] += 1\n",
    "                alpha_t = 1 / N[state][action]  # Decaying alpha\n",
    "                q_table[state][action] += alpha_t * (G - q_table[state][action])\n",
    "\n",
    "    elif args.model_name == 'TD':\n",
    "        alpha = args.alpha  # Learning rate\n",
    "        gamma = args.gamma  # Discount factor\n",
    "        epsilon = args.epsilon  # Exploration rate\n",
    "        num_episodes = args.num_episodes\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = epsilon_greedy_policy(state, env, epsilon, q_table)  # Epsilon-greedy action selection\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                # TD(0) update\n",
    "                old_value = q_table[state, action]\n",
    "                next_max = np.max(q_table[next_state])\n",
    "                new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "                q_table[state, action] = new_value\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "\n",
    "            rewards_per_episode.append(episode_rewards)\n",
    "\n",
    "    elif args.model_name == 'sarsa':\n",
    "        alpha = args.alpha  # Learning rate\n",
    "        gamma = args.gamma  # Discount factor\n",
    "        epsilon = args.epsilon  # Exploration rate\n",
    "\n",
    "        num_episodes = args.num_episodes\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            action = epsilon_greedy_policy(state, env, epsilon, q_table)\n",
    "            done = False\n",
    "            episode_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_action = epsilon_greedy_policy(next_state, env, epsilon, q_table)\n",
    "\n",
    "                # Update Q-value using the SARSA update rule\n",
    "                old_value = q_table[state, action]\n",
    "                next_value = q_table[next_state, next_action]\n",
    "                new_value = old_value + alpha * (reward + gamma * next_value - old_value)\n",
    "                q_table[state, action] = new_value\n",
    "\n",
    "                state, action = next_state, next_action\n",
    "                episode_rewards += reward\n",
    "\n",
    "            rewards_per_episode.append(episode_rewards)\n",
    "    elif args.model_name == 'dqn':\n",
    "        input_dim = env.observation_space.n\n",
    "        output_dim = env.action_space.n\n",
    "\n",
    "        dqn = DQN(input_dim, output_dim)\n",
    "        optimizer = optim.Adam(dqn.parameters(), lr=args.alpha)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        gamma = args.gamma\n",
    "        epsilon = args.epsilon\n",
    "        num_episodes = args.num_episodes\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            print(f\"episode {episode}\")\n",
    "            state = env.reset()[0]\n",
    "            original_state = state\n",
    "            state = torch.tensor(one_hot_encode(state, input_dim), dtype=torch.float32)\n",
    "            done = False\n",
    "            episode_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                q_values = dqn(state)\n",
    "                action = epsilon_greedy_policy(state, env, epsilon, q_values.detach().cpu().numpy(), is_dqn=True)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_state = torch.tensor(one_hot_encode(next_state, input_dim), dtype=torch.float32)\n",
    "\n",
    "                target = q_values.clone()\n",
    "                if done:\n",
    "                    target[action] = reward\n",
    "                else:\n",
    "                    next_q_values = dqn(next_state)\n",
    "                    target[action] = reward + gamma * torch.max(next_q_values).item()\n",
    "\n",
    "                loss = loss_fn(q_values, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "\n",
    "            rewards_per_episode.append(episode_rewards)\n",
    "\n",
    "    with open(f\"models/{args.model_name}_{args.env_name}_q_table.pkl\", \"wb\") as f:\n",
    "        pickle.dump(q_table, f)\n",
    "\n",
    "    return q_table, rewards_per_episode\n",
    "\n",
    "\n",
    "def test(args, num_episodes=1):\n",
    "    print(\"test start\")\n",
    "    if args.env_name == 'frozenlake':\n",
    "        env = gym.make('FrozenLake-v1', render_mode='human', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "    elif args.env_name == 'cliffwalking':\n",
    "        env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "    elif args.env_name == 'taxi':\n",
    "        env = gym.make('Taxi-v3', render_mode='human')\n",
    "\n",
    "    with open(f\"models/{args.model_name}_{args.env_name}_q_table.pkl\", \"rb\") as f:\n",
    "        q_table = pickle.load(f)\n",
    "\n",
    "    total_rewards = 0\n",
    "    success_count = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            episode_rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                print(\"done!!\",reward)\n",
    "                if args.env_name =='cliffwalking':\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    if reward > 0:\n",
    "                        success_count += 1\n",
    "\n",
    "        total_rewards += episode_rewards\n",
    "        total_steps += steps\n",
    "\n",
    "    avg_rewards = total_rewards / num_episodes\n",
    "    success_rate = success_count / num_episodes\n",
    "    avg_steps = total_steps / num_episodes\n",
    "\n",
    "    print(f\"Average Rewards: {avg_rewards}\")\n",
    "    print(f\"Success Rate: {success_rate * 100}%\")\n",
    "    print(f\"Average Steps to Goal: {avg_steps}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_rewards(env_list, model_list, all_rewards):\n",
    "    fig, axes = plt.subplots(len(env_list), len(model_list), figsize=(15, 15))\n",
    "    fig.suptitle('Rewards per Episode for Different Environments and Models', fontsize=16)\n",
    "\n",
    "    for i, env in enumerate(env_list):\n",
    "        for j, model in enumerate(model_list):\n",
    "            rewards = all_rewards[i][j]\n",
    "            axes[i, j].plot(rewards)\n",
    "            axes[i, j].set_title(f'{env} - {model}')\n",
    "            axes[i, j].set_xlabel('Episodes')\n",
    "            axes[i, j].set_ylabel('Rewards')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', type=str, default='q_learning', help='Name of the model')\n",
    "    parser.add_argument('--env_name', type=str, default='cliffwalking', help='Name of the env')\n",
    "    parser.add_argument('--alpha', type=float, default=0.1, help='Learning rate')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='Discount factor')\n",
    "    parser.add_argument('--epsilon', type=float, default=0.3, help='Exploration rate')\n",
    "    parser.add_argument('--num_episodes', type=int, default=30000, help='Number of training episodes')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    env_list = ['cliffwalking', 'frozenlake', 'taxi']\n",
    "    model_list = [\"TD\",\"q_learning\", \"sarsa\" ]  # \"TD\" \"MC\" \"q_learning\", \"sarsa\" , \"dqn\"\n",
    "\n",
    "    all_rewards = []\n",
    "\n",
    "    for env in env_list:\n",
    "        print(f\"env is {env}\")\n",
    "        model_rewards_per_episode = []\n",
    "        for model in model_list:\n",
    "            print(f\"model is {model}\")\n",
    "            args.env_name = env\n",
    "            args.model_name = model\n",
    "            _, rewards_per_episode = train(args)\n",
    "            test(args)\n",
    "            model_rewards_per_episode.append(rewards_per_episode)\n",
    "        all_rewards.append(model_rewards_per_episode)\n",
    "\n",
    "    plot_rewards(env_list, model_list, all_rewards)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
